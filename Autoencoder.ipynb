{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/root/Desktop/vgg16/trainingSet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global params\n",
    "params = {'path' : path,\n",
    "          'batch_size' : 50,\n",
    "          'input_size': (32,32),\n",
    "          'encoded_size': 8,   \n",
    "          'lrn_rate': 0.0001, \n",
    "          'epochs': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(path):\n",
    "    paths = sorted([os.path.join(root, file)  for root, dirs, files in os.walk(path) for file in files])\n",
    "    paths = shuffle(paths)\n",
    "    return paths[:int(len(paths)*.8)], paths[int(len(paths)*.8):]  #trainpaths, testpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    img = cv2.imread(path, 0)\n",
    "#     if img is None: return (False, None)\n",
    "    img = cv2.resize(cv2.threshold(img, 240, 1, cv2.THRESH_BINARY)[1], \n",
    "                     params['input_size'], # <-- input_size tuple of format (h,w)\n",
    "                     cv2.INTER_AREA)\n",
    "    return np.expand_dims(img, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "global conv, convt, maxpool, upsample, Relu\n",
    "conv = tf.layers.conv2d\n",
    "convt = tf.layers.conv2d_transpose\n",
    "maxpool = tf.layers.max_pooling2d\n",
    "upsample = tf.image.resize_nearest_neighbor\n",
    "Relu = tf.nn.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input):\n",
    "    with tf.variable_scope('encoder') as scope:\n",
    "        net = conv(input, filters=32, kernel_size=3, padding='SAME', strides=1, activation=Relu, name='Save_C1_1')\n",
    "        net = conv(net, filters=32, kernel_size=3, padding='SAME', strides=1, activation=Relu, name='Save_C1_2')\n",
    "        net = conv(net, filters=32, kernel_size=3, padding='SAME', strides=1, activation=Relu, name='Save_C1_3')\n",
    "        net = maxpool(net, pool_size=2, padding='SAME', strides=2, name='M1')\n",
    "\n",
    "        net = conv(net, filters=64, kernel_size=3, padding='SAME',  strides=1, activation=Relu, name='Save_C2_1')\n",
    "        net = conv(net, filters=64, kernel_size=3, padding='SAME', strides=1, activation=Relu, name='Save_C2_2')\n",
    "        net = conv(net, filters=64, kernel_size=3, padding='SAME', strides=1, activation=Relu, name='Save_C2_3')\n",
    "        net = maxpool(net, pool_size=2, padding='SAME', strides=2, name='M2')\n",
    "\n",
    "        net = conv(net, filters=64, kernel_size=3, padding='SAME', strides=1, activation=Relu, name='C3_1')\n",
    "        net = conv(net, filters=64, kernel_size=3, padding='SAME', strides=1, activation=Relu, name='C3_2')\n",
    "        encoded = conv(net, filters=64, kernel_size=3, padding='SAME', strides=1, activation=Relu, name='C3_3')\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decoder(encoded):\n",
    "    with tf.variable_scope('decoder') as scope:\n",
    "        net = upsample(encoded, size=(16,16))\n",
    "        net = convt(net, filters=32, kernel_size=3, padding='SAME', strides=1, activation=Relu, name='TrC1_1')\n",
    "        net = convt(net, filters=32, kernel_size=3, padding='SAME', strides=1, activation=Relu, name='TrC1_2')\n",
    "        net = convt(net, filters=32, kernel_size=3, padding='SAME', strides=1, activation=Relu, name='TrC1_3')\n",
    "\n",
    "        net = upsample(net, size=(32,32))\n",
    "        decoded = convt(tf.layers.batch_normalization(net), filters=1, kernel_size=3, padding='SAME', strides=1, activation=Relu, name='TrC2')\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, gen):\n",
    "    with tf.variable_scope('L2_loss') as scope:\n",
    "        loss = tf.reduce_mean(tf.square(real - gen))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Autoencoder_Model():\n",
    "\n",
    "    batchsize = params['batch_size']\n",
    "    \n",
    "    X = tf.placeholder(dtype = tf.float32, shape = [batchsize, 32, 32, 1])\n",
    "    dec = decoder(encoder(X))\n",
    "    loss = loss_function(X, dec)\n",
    "    optimiz = tf.train.AdadeltaOptimizer(params['lrn_rate']).minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    tf.summary.scalar('Loss_Value',loss)\n",
    "    real = tf.summary.image(\"Input\", X, max_outputs=2)\n",
    "    generated = tf.summary.image(\"Output\", dec, max_outputs=2)\n",
    "    print('Setting up summary op...')\n",
    "    summary_op = tf.summary.merge_all()\n",
    "   \n",
    "    print(\"Setting up Saver...\") \n",
    "    saver = tf.train.Saver() \n",
    "    train_summary_writer = tf.summary.FileWriter('./log_dirAE/train', sess.graph)\n",
    "    test_summary_writer = tf.summary.FileWriter('./log_dirAE/test', sess.graph)\n",
    "    \n",
    "    train_paths, test_paths = get_file_paths(params['path'])\n",
    "    \n",
    "    dataset_size = len(train_paths)\n",
    "    n_batches = dataset_size//batchsize\n",
    "    print(\"Dataset Size %d \\nNo of Batches %d \" %(dataset_size,n_batches))\n",
    "    batch_data, test_batch_data = [], []\n",
    "    \n",
    "    counter = 1\n",
    "    for epoch in range(params['epochs']):\n",
    "\n",
    "        for idx in range(n_batches): \n",
    "            \n",
    "            batch_paths = train_paths[idx * batchsize: (idx+1) * batchsize]\n",
    "            batch_data = np.array([load_data(path) for path in batch_paths])\n",
    "#             print(\"Train Batch shape \", str(batch_data.shape)) \n",
    "  \n",
    "            _, train_loss, train_summary_str = sess.run([optimiz, loss, summary_op], \n",
    "                                                  feed_dict= {X: batch_data})\n",
    "            train_summary_writer.add_summary(train_summary_str, counter)\n",
    "            \n",
    "            if idx % 10 == 0:\n",
    "                print(\"Epoch %d \\t Batch %d \\t Loss %.4f\"%(epoch, idx+1, train_loss))\n",
    "                \n",
    "                #Choosing random n=batchsize paths from 'test_paths' list\n",
    "                test_batch_paths = np.array(test_paths)[np.random.choice(len(test_paths), batchsize)]\n",
    "                test_batch_data = np.array([load_data(path) for path in test_batch_paths])\n",
    "#                 print(\"Test Batch shape \", str(test_batch_data.shape))\n",
    "                _, test_summary_str = sess.run([dec, summary_op], \n",
    "                                               feed_dict= {X: test_batch_data})\n",
    "                \n",
    "                test_summary_writer.add_summary(test_summary_str, counter)\n",
    "            if counter %100 == 0:\n",
    "                saver.save(sess, './log_dir_bangla_recov_weights/' + \"model.ckpt\", counter)\n",
    "            \n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up summary op...\n",
      "Setting up Saver...\n",
      "Dataset Size 33600 \n",
      "No of Batches 672 \n",
      "Epoch 0 \t Batch 1 \t Loss 0.0641\n",
      "Epoch 0 \t Batch 11 \t Loss 0.0608\n",
      "Epoch 0 \t Batch 21 \t Loss 0.0614\n",
      "Epoch 0 \t Batch 31 \t Loss 0.0609\n",
      "Epoch 0 \t Batch 41 \t Loss 0.0627\n",
      "Epoch 0 \t Batch 51 \t Loss 0.0667\n",
      "Epoch 0 \t Batch 61 \t Loss 0.0654\n",
      "Epoch 0 \t Batch 71 \t Loss 0.0601\n",
      "Epoch 0 \t Batch 81 \t Loss 0.0637\n",
      "Epoch 0 \t Batch 91 \t Loss 0.0535\n",
      "Epoch 0 \t Batch 101 \t Loss 0.0673\n",
      "Epoch 0 \t Batch 111 \t Loss 0.0645\n",
      "Epoch 0 \t Batch 121 \t Loss 0.0685\n",
      "Epoch 0 \t Batch 131 \t Loss 0.0709\n",
      "Epoch 0 \t Batch 141 \t Loss 0.0641\n",
      "Epoch 0 \t Batch 151 \t Loss 0.0566\n",
      "Epoch 0 \t Batch 161 \t Loss 0.0601\n",
      "Epoch 0 \t Batch 171 \t Loss 0.0677\n",
      "Epoch 0 \t Batch 181 \t Loss 0.0603\n",
      "Epoch 0 \t Batch 191 \t Loss 0.0560\n",
      "Epoch 0 \t Batch 201 \t Loss 0.0640\n",
      "Epoch 0 \t Batch 211 \t Loss 0.0623\n",
      "Epoch 0 \t Batch 221 \t Loss 0.0588\n",
      "Epoch 0 \t Batch 231 \t Loss 0.0590\n",
      "Epoch 0 \t Batch 241 \t Loss 0.0636\n",
      "Epoch 0 \t Batch 251 \t Loss 0.0690\n",
      "Epoch 0 \t Batch 261 \t Loss 0.0700\n",
      "Epoch 0 \t Batch 271 \t Loss 0.0603\n",
      "Epoch 0 \t Batch 281 \t Loss 0.0679\n",
      "Epoch 0 \t Batch 291 \t Loss 0.0629\n",
      "Epoch 0 \t Batch 301 \t Loss 0.0628\n",
      "Epoch 0 \t Batch 311 \t Loss 0.0618\n",
      "Epoch 0 \t Batch 321 \t Loss 0.0645\n",
      "Epoch 0 \t Batch 331 \t Loss 0.0656\n",
      "Epoch 0 \t Batch 341 \t Loss 0.0612\n",
      "Epoch 0 \t Batch 351 \t Loss 0.0674\n",
      "Epoch 0 \t Batch 361 \t Loss 0.0683\n",
      "Epoch 0 \t Batch 371 \t Loss 0.0670\n",
      "Epoch 0 \t Batch 381 \t Loss 0.0613\n",
      "Epoch 0 \t Batch 391 \t Loss 0.0587\n",
      "Epoch 0 \t Batch 401 \t Loss 0.0643\n",
      "Epoch 0 \t Batch 411 \t Loss 0.0709\n",
      "Epoch 0 \t Batch 421 \t Loss 0.0690\n",
      "Epoch 0 \t Batch 431 \t Loss 0.0656\n",
      "Epoch 0 \t Batch 441 \t Loss 0.0668\n",
      "Epoch 0 \t Batch 451 \t Loss 0.0666\n",
      "Epoch 0 \t Batch 461 \t Loss 0.0670\n",
      "Epoch 0 \t Batch 471 \t Loss 0.0617\n",
      "Epoch 0 \t Batch 481 \t Loss 0.0723\n",
      "Epoch 0 \t Batch 491 \t Loss 0.0662\n",
      "Epoch 0 \t Batch 501 \t Loss 0.0770\n",
      "Epoch 0 \t Batch 511 \t Loss 0.0664\n",
      "Epoch 0 \t Batch 521 \t Loss 0.0718\n",
      "Epoch 0 \t Batch 531 \t Loss 0.0624\n",
      "Epoch 0 \t Batch 541 \t Loss 0.0694\n",
      "Epoch 0 \t Batch 551 \t Loss 0.0581\n",
      "Epoch 0 \t Batch 561 \t Loss 0.0608\n",
      "Epoch 0 \t Batch 571 \t Loss 0.0667\n",
      "Epoch 0 \t Batch 581 \t Loss 0.0643\n",
      "Epoch 0 \t Batch 591 \t Loss 0.0644\n",
      "Epoch 0 \t Batch 601 \t Loss 0.0590\n",
      "Epoch 0 \t Batch 611 \t Loss 0.0556\n",
      "Epoch 0 \t Batch 621 \t Loss 0.0569\n",
      "Epoch 0 \t Batch 631 \t Loss 0.0612\n",
      "Epoch 0 \t Batch 641 \t Loss 0.0682\n",
      "Epoch 0 \t Batch 651 \t Loss 0.0582\n",
      "Epoch 0 \t Batch 661 \t Loss 0.0634\n",
      "Epoch 0 \t Batch 671 \t Loss 0.0557\n",
      "Epoch 1 \t Batch 1 \t Loss 0.0639\n",
      "Epoch 1 \t Batch 11 \t Loss 0.0607\n",
      "Epoch 1 \t Batch 21 \t Loss 0.0613\n",
      "Epoch 1 \t Batch 31 \t Loss 0.0608\n",
      "Epoch 1 \t Batch 41 \t Loss 0.0625\n",
      "Epoch 1 \t Batch 51 \t Loss 0.0665\n",
      "Epoch 1 \t Batch 61 \t Loss 0.0652\n",
      "Epoch 1 \t Batch 71 \t Loss 0.0599\n",
      "Epoch 1 \t Batch 81 \t Loss 0.0636\n",
      "Epoch 1 \t Batch 91 \t Loss 0.0534\n",
      "Epoch 1 \t Batch 101 \t Loss 0.0671\n",
      "Epoch 1 \t Batch 111 \t Loss 0.0644\n",
      "Epoch 1 \t Batch 121 \t Loss 0.0684\n",
      "Epoch 1 \t Batch 131 \t Loss 0.0708\n",
      "Epoch 1 \t Batch 141 \t Loss 0.0640\n",
      "Epoch 1 \t Batch 151 \t Loss 0.0565\n",
      "Epoch 1 \t Batch 161 \t Loss 0.0600\n",
      "Epoch 1 \t Batch 171 \t Loss 0.0676\n",
      "Epoch 1 \t Batch 181 \t Loss 0.0601\n",
      "Epoch 1 \t Batch 191 \t Loss 0.0559\n",
      "Epoch 1 \t Batch 201 \t Loss 0.0638\n",
      "Epoch 1 \t Batch 211 \t Loss 0.0621\n",
      "Epoch 1 \t Batch 221 \t Loss 0.0587\n",
      "Epoch 1 \t Batch 231 \t Loss 0.0589\n",
      "Epoch 1 \t Batch 241 \t Loss 0.0634\n",
      "Epoch 1 \t Batch 251 \t Loss 0.0689\n",
      "Epoch 1 \t Batch 261 \t Loss 0.0699\n",
      "Epoch 1 \t Batch 271 \t Loss 0.0601\n",
      "Epoch 1 \t Batch 281 \t Loss 0.0677\n",
      "Epoch 1 \t Batch 291 \t Loss 0.0627\n",
      "Epoch 1 \t Batch 301 \t Loss 0.0627\n",
      "Epoch 1 \t Batch 311 \t Loss 0.0616\n",
      "Epoch 1 \t Batch 321 \t Loss 0.0643\n",
      "Epoch 1 \t Batch 331 \t Loss 0.0655\n",
      "Epoch 1 \t Batch 341 \t Loss 0.0610\n",
      "Epoch 1 \t Batch 351 \t Loss 0.0673\n",
      "Epoch 1 \t Batch 361 \t Loss 0.0681\n",
      "Epoch 1 \t Batch 371 \t Loss 0.0668\n",
      "Epoch 1 \t Batch 381 \t Loss 0.0612\n",
      "Epoch 1 \t Batch 391 \t Loss 0.0586\n",
      "Epoch 1 \t Batch 401 \t Loss 0.0641\n",
      "Epoch 1 \t Batch 411 \t Loss 0.0708\n",
      "Epoch 1 \t Batch 421 \t Loss 0.0688\n",
      "Epoch 1 \t Batch 431 \t Loss 0.0654\n",
      "Epoch 1 \t Batch 441 \t Loss 0.0666\n",
      "Epoch 1 \t Batch 451 \t Loss 0.0664\n",
      "Epoch 1 \t Batch 461 \t Loss 0.0668\n",
      "Epoch 1 \t Batch 471 \t Loss 0.0615\n",
      "Epoch 1 \t Batch 481 \t Loss 0.0721\n",
      "Epoch 1 \t Batch 491 \t Loss 0.0660\n",
      "Epoch 1 \t Batch 501 \t Loss 0.0768\n",
      "Epoch 1 \t Batch 511 \t Loss 0.0662\n",
      "Epoch 1 \t Batch 521 \t Loss 0.0716\n",
      "Epoch 1 \t Batch 531 \t Loss 0.0623\n",
      "Epoch 1 \t Batch 541 \t Loss 0.0692\n",
      "Epoch 1 \t Batch 551 \t Loss 0.0579\n",
      "Epoch 1 \t Batch 561 \t Loss 0.0606\n",
      "Epoch 1 \t Batch 571 \t Loss 0.0666\n",
      "Epoch 1 \t Batch 581 \t Loss 0.0641\n",
      "Epoch 1 \t Batch 591 \t Loss 0.0642\n",
      "Epoch 1 \t Batch 601 \t Loss 0.0589\n",
      "Epoch 1 \t Batch 611 \t Loss 0.0554\n",
      "Epoch 1 \t Batch 621 \t Loss 0.0568\n",
      "Epoch 1 \t Batch 631 \t Loss 0.0611\n",
      "Epoch 1 \t Batch 641 \t Loss 0.0681\n",
      "Epoch 1 \t Batch 651 \t Loss 0.0580\n",
      "Epoch 1 \t Batch 661 \t Loss 0.0633\n",
      "Epoch 1 \t Batch 671 \t Loss 0.0555\n",
      "Epoch 2 \t Batch 1 \t Loss 0.0638\n",
      "Epoch 2 \t Batch 11 \t Loss 0.0605\n",
      "Epoch 2 \t Batch 21 \t Loss 0.0611\n",
      "Epoch 2 \t Batch 31 \t Loss 0.0607\n",
      "Epoch 2 \t Batch 41 \t Loss 0.0624\n",
      "Epoch 2 \t Batch 51 \t Loss 0.0663\n",
      "Epoch 2 \t Batch 61 \t Loss 0.0650\n",
      "Epoch 2 \t Batch 71 \t Loss 0.0598\n",
      "Epoch 2 \t Batch 81 \t Loss 0.0634\n",
      "Epoch 2 \t Batch 91 \t Loss 0.0532\n",
      "Epoch 2 \t Batch 101 \t Loss 0.0670\n",
      "Epoch 2 \t Batch 111 \t Loss 0.0642\n",
      "Epoch 2 \t Batch 121 \t Loss 0.0682\n",
      "Epoch 2 \t Batch 131 \t Loss 0.0706\n",
      "Epoch 2 \t Batch 141 \t Loss 0.0638\n",
      "Epoch 2 \t Batch 151 \t Loss 0.0564\n",
      "Epoch 2 \t Batch 161 \t Loss 0.0598\n",
      "Epoch 2 \t Batch 171 \t Loss 0.0674\n",
      "Epoch 2 \t Batch 181 \t Loss 0.0600\n",
      "Epoch 2 \t Batch 191 \t Loss 0.0557\n",
      "Epoch 2 \t Batch 201 \t Loss 0.0636\n",
      "Epoch 2 \t Batch 211 \t Loss 0.0620\n",
      "Epoch 2 \t Batch 221 \t Loss 0.0585\n",
      "Epoch 2 \t Batch 231 \t Loss 0.0587\n",
      "Epoch 2 \t Batch 241 \t Loss 0.0632\n",
      "Epoch 2 \t Batch 251 \t Loss 0.0687\n",
      "Epoch 2 \t Batch 261 \t Loss 0.0697\n",
      "Epoch 2 \t Batch 271 \t Loss 0.0600\n",
      "Epoch 2 \t Batch 281 \t Loss 0.0675\n",
      "Epoch 2 \t Batch 291 \t Loss 0.0626\n",
      "Epoch 2 \t Batch 301 \t Loss 0.0625\n",
      "Epoch 2 \t Batch 311 \t Loss 0.0615\n",
      "Epoch 2 \t Batch 321 \t Loss 0.0641\n",
      "Epoch 2 \t Batch 331 \t Loss 0.0653\n",
      "Epoch 2 \t Batch 341 \t Loss 0.0609\n",
      "Epoch 2 \t Batch 351 \t Loss 0.0671\n",
      "Epoch 2 \t Batch 361 \t Loss 0.0679\n",
      "Epoch 2 \t Batch 371 \t Loss 0.0666\n",
      "Epoch 2 \t Batch 381 \t Loss 0.0610\n",
      "Epoch 2 \t Batch 391 \t Loss 0.0584\n",
      "Epoch 2 \t Batch 401 \t Loss 0.0639\n",
      "Epoch 2 \t Batch 411 \t Loss 0.0705\n",
      "Epoch 2 \t Batch 421 \t Loss 0.0686\n",
      "Epoch 2 \t Batch 431 \t Loss 0.0652\n",
      "Epoch 2 \t Batch 441 \t Loss 0.0664\n",
      "Epoch 2 \t Batch 451 \t Loss 0.0662\n",
      "Epoch 2 \t Batch 461 \t Loss 0.0666\n",
      "Epoch 2 \t Batch 471 \t Loss 0.0613\n",
      "Epoch 2 \t Batch 481 \t Loss 0.0719\n",
      "Epoch 2 \t Batch 491 \t Loss 0.0658\n",
      "Epoch 2 \t Batch 501 \t Loss 0.0766\n",
      "Epoch 2 \t Batch 511 \t Loss 0.0660\n",
      "Epoch 2 \t Batch 521 \t Loss 0.0713\n",
      "Epoch 2 \t Batch 531 \t Loss 0.0621\n",
      "Epoch 2 \t Batch 541 \t Loss 0.0690\n",
      "Epoch 2 \t Batch 551 \t Loss 0.0578\n",
      "Epoch 2 \t Batch 561 \t Loss 0.0604\n",
      "Epoch 2 \t Batch 571 \t Loss 0.0664\n",
      "Epoch 2 \t Batch 581 \t Loss 0.0639\n",
      "Epoch 2 \t Batch 591 \t Loss 0.0640\n",
      "Epoch 2 \t Batch 601 \t Loss 0.0587\n",
      "Epoch 2 \t Batch 611 \t Loss 0.0553\n",
      "Epoch 2 \t Batch 621 \t Loss 0.0566\n",
      "Epoch 2 \t Batch 631 \t Loss 0.0609\n",
      "Epoch 2 \t Batch 641 \t Loss 0.0678\n",
      "Epoch 2 \t Batch 651 \t Loss 0.0579\n",
      "Epoch 2 \t Batch 661 \t Loss 0.0631\n",
      "Epoch 2 \t Batch 671 \t Loss 0.0554\n",
      "Epoch 3 \t Batch 1 \t Loss 0.0636\n",
      "Epoch 3 \t Batch 11 \t Loss 0.0603\n",
      "Epoch 3 \t Batch 21 \t Loss 0.0609\n",
      "Epoch 3 \t Batch 31 \t Loss 0.0605\n",
      "Epoch 3 \t Batch 41 \t Loss 0.0622\n",
      "Epoch 3 \t Batch 51 \t Loss 0.0661\n",
      "Epoch 3 \t Batch 61 \t Loss 0.0648\n",
      "Epoch 3 \t Batch 71 \t Loss 0.0596\n",
      "Epoch 3 \t Batch 81 \t Loss 0.0632\n",
      "Epoch 3 \t Batch 91 \t Loss 0.0531\n",
      "Epoch 3 \t Batch 101 \t Loss 0.0667\n",
      "Epoch 3 \t Batch 111 \t Loss 0.0640\n",
      "Epoch 3 \t Batch 121 \t Loss 0.0680\n",
      "Epoch 3 \t Batch 131 \t Loss 0.0704\n",
      "Epoch 3 \t Batch 141 \t Loss 0.0636\n",
      "Epoch 3 \t Batch 151 \t Loss 0.0562\n",
      "Epoch 3 \t Batch 161 \t Loss 0.0596\n",
      "Epoch 3 \t Batch 171 \t Loss 0.0672\n",
      "Epoch 3 \t Batch 181 \t Loss 0.0598\n",
      "Epoch 3 \t Batch 191 \t Loss 0.0555\n",
      "Epoch 3 \t Batch 201 \t Loss 0.0634\n",
      "Epoch 3 \t Batch 211 \t Loss 0.0617\n",
      "Epoch 3 \t Batch 221 \t Loss 0.0584\n",
      "Epoch 3 \t Batch 231 \t Loss 0.0585\n",
      "Epoch 3 \t Batch 241 \t Loss 0.0630\n",
      "Epoch 3 \t Batch 251 \t Loss 0.0684\n",
      "Epoch 3 \t Batch 261 \t Loss 0.0694\n",
      "Epoch 3 \t Batch 271 \t Loss 0.0598\n",
      "Epoch 3 \t Batch 281 \t Loss 0.0673\n",
      "Epoch 3 \t Batch 291 \t Loss 0.0624\n",
      "Epoch 3 \t Batch 301 \t Loss 0.0623\n",
      "Epoch 3 \t Batch 311 \t Loss 0.0613\n",
      "Epoch 3 \t Batch 321 \t Loss 0.0639\n",
      "Epoch 3 \t Batch 331 \t Loss 0.0651\n",
      "Epoch 3 \t Batch 341 \t Loss 0.0607\n",
      "Epoch 3 \t Batch 351 \t Loss 0.0668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 \t Batch 361 \t Loss 0.0677\n",
      "Epoch 3 \t Batch 371 \t Loss 0.0664\n",
      "Epoch 3 \t Batch 381 \t Loss 0.0608\n",
      "Epoch 3 \t Batch 391 \t Loss 0.0582\n",
      "Epoch 3 \t Batch 401 \t Loss 0.0637\n",
      "Epoch 3 \t Batch 411 \t Loss 0.0703\n",
      "Epoch 3 \t Batch 421 \t Loss 0.0684\n",
      "Epoch 3 \t Batch 431 \t Loss 0.0650\n",
      "Epoch 3 \t Batch 441 \t Loss 0.0662\n",
      "Epoch 3 \t Batch 451 \t Loss 0.0660\n",
      "Epoch 3 \t Batch 461 \t Loss 0.0664\n",
      "Epoch 3 \t Batch 471 \t Loss 0.0611\n",
      "Epoch 3 \t Batch 481 \t Loss 0.0717\n",
      "Epoch 3 \t Batch 491 \t Loss 0.0656\n",
      "Epoch 3 \t Batch 501 \t Loss 0.0763\n",
      "Epoch 3 \t Batch 511 \t Loss 0.0658\n",
      "Epoch 3 \t Batch 521 \t Loss 0.0711\n",
      "Epoch 3 \t Batch 531 \t Loss 0.0618\n",
      "Epoch 3 \t Batch 541 \t Loss 0.0688\n",
      "Epoch 3 \t Batch 551 \t Loss 0.0576\n",
      "Epoch 3 \t Batch 561 \t Loss 0.0602\n",
      "Epoch 3 \t Batch 571 \t Loss 0.0661\n",
      "Epoch 3 \t Batch 581 \t Loss 0.0637\n",
      "Epoch 3 \t Batch 591 \t Loss 0.0638\n",
      "Epoch 3 \t Batch 601 \t Loss 0.0585\n",
      "Epoch 3 \t Batch 611 \t Loss 0.0551\n",
      "Epoch 3 \t Batch 621 \t Loss 0.0564\n",
      "Epoch 3 \t Batch 631 \t Loss 0.0606\n",
      "Epoch 3 \t Batch 641 \t Loss 0.0676\n",
      "Epoch 3 \t Batch 651 \t Loss 0.0576\n",
      "Epoch 3 \t Batch 661 \t Loss 0.0628\n",
      "Epoch 3 \t Batch 671 \t Loss 0.0552\n",
      "Epoch 4 \t Batch 1 \t Loss 0.0633\n",
      "Epoch 4 \t Batch 11 \t Loss 0.0601\n",
      "Epoch 4 \t Batch 21 \t Loss 0.0607\n",
      "Epoch 4 \t Batch 31 \t Loss 0.0602\n",
      "Epoch 4 \t Batch 41 \t Loss 0.0619\n",
      "Epoch 4 \t Batch 51 \t Loss 0.0659\n",
      "Epoch 4 \t Batch 61 \t Loss 0.0646\n",
      "Epoch 4 \t Batch 71 \t Loss 0.0594\n",
      "Epoch 4 \t Batch 81 \t Loss 0.0630\n",
      "Epoch 4 \t Batch 91 \t Loss 0.0529\n",
      "Epoch 4 \t Batch 101 \t Loss 0.0665\n",
      "Epoch 4 \t Batch 111 \t Loss 0.0638\n",
      "Epoch 4 \t Batch 121 \t Loss 0.0677\n",
      "Epoch 4 \t Batch 131 \t Loss 0.0701\n",
      "Epoch 4 \t Batch 141 \t Loss 0.0634\n",
      "Epoch 4 \t Batch 151 \t Loss 0.0560\n",
      "Epoch 4 \t Batch 161 \t Loss 0.0594\n",
      "Epoch 4 \t Batch 171 \t Loss 0.0669\n",
      "Epoch 4 \t Batch 181 \t Loss 0.0596\n",
      "Epoch 4 \t Batch 191 \t Loss 0.0553\n",
      "Epoch 4 \t Batch 201 \t Loss 0.0632\n",
      "Epoch 4 \t Batch 211 \t Loss 0.0615\n",
      "Epoch 4 \t Batch 221 \t Loss 0.0581\n",
      "Epoch 4 \t Batch 231 \t Loss 0.0583\n",
      "Epoch 4 \t Batch 241 \t Loss 0.0628\n",
      "Epoch 4 \t Batch 251 \t Loss 0.0682\n",
      "Epoch 4 \t Batch 261 \t Loss 0.0692\n",
      "Epoch 4 \t Batch 271 \t Loss 0.0595\n",
      "Epoch 4 \t Batch 281 \t Loss 0.0670\n",
      "Epoch 4 \t Batch 291 \t Loss 0.0621\n",
      "Epoch 4 \t Batch 301 \t Loss 0.0620\n",
      "Epoch 4 \t Batch 311 \t Loss 0.0610\n",
      "Epoch 4 \t Batch 321 \t Loss 0.0637\n",
      "Epoch 4 \t Batch 331 \t Loss 0.0648\n",
      "Epoch 4 \t Batch 341 \t Loss 0.0604\n",
      "Epoch 4 \t Batch 351 \t Loss 0.0666\n",
      "Epoch 4 \t Batch 361 \t Loss 0.0674\n",
      "Epoch 4 \t Batch 371 \t Loss 0.0662\n",
      "Epoch 4 \t Batch 381 \t Loss 0.0605\n",
      "Epoch 4 \t Batch 391 \t Loss 0.0580\n",
      "Epoch 4 \t Batch 401 \t Loss 0.0635\n",
      "Epoch 4 \t Batch 411 \t Loss 0.0700\n",
      "Epoch 4 \t Batch 421 \t Loss 0.0681\n",
      "Epoch 4 \t Batch 431 \t Loss 0.0647\n",
      "Epoch 4 \t Batch 441 \t Loss 0.0659\n",
      "Epoch 4 \t Batch 451 \t Loss 0.0657\n",
      "Epoch 4 \t Batch 461 \t Loss 0.0661\n",
      "Epoch 4 \t Batch 471 \t Loss 0.0609\n",
      "Epoch 4 \t Batch 481 \t Loss 0.0714\n",
      "Epoch 4 \t Batch 491 \t Loss 0.0653\n",
      "Epoch 4 \t Batch 501 \t Loss 0.0760\n",
      "Epoch 4 \t Batch 511 \t Loss 0.0655\n",
      "Epoch 4 \t Batch 521 \t Loss 0.0708\n",
      "Epoch 4 \t Batch 531 \t Loss 0.0616\n",
      "Epoch 4 \t Batch 541 \t Loss 0.0685\n",
      "Epoch 4 \t Batch 551 \t Loss 0.0573\n",
      "Epoch 4 \t Batch 561 \t Loss 0.0600\n",
      "Epoch 4 \t Batch 571 \t Loss 0.0659\n",
      "Epoch 4 \t Batch 581 \t Loss 0.0635\n",
      "Epoch 4 \t Batch 591 \t Loss 0.0636\n",
      "Epoch 4 \t Batch 601 \t Loss 0.0583\n",
      "Epoch 4 \t Batch 611 \t Loss 0.0549\n",
      "Epoch 4 \t Batch 621 \t Loss 0.0562\n",
      "Epoch 4 \t Batch 631 \t Loss 0.0604\n",
      "Epoch 4 \t Batch 641 \t Loss 0.0673\n",
      "Epoch 4 \t Batch 651 \t Loss 0.0574\n",
      "Epoch 4 \t Batch 661 \t Loss 0.0626\n",
      "Epoch 4 \t Batch 671 \t Loss 0.0550\n",
      "Epoch 5 \t Batch 1 \t Loss 0.0631\n",
      "Epoch 5 \t Batch 11 \t Loss 0.0599\n",
      "Epoch 5 \t Batch 21 \t Loss 0.0604\n",
      "Epoch 5 \t Batch 31 \t Loss 0.0600\n",
      "Epoch 5 \t Batch 41 \t Loss 0.0617\n",
      "Epoch 5 \t Batch 51 \t Loss 0.0656\n",
      "Epoch 5 \t Batch 61 \t Loss 0.0643\n",
      "Epoch 5 \t Batch 71 \t Loss 0.0591\n",
      "Epoch 5 \t Batch 81 \t Loss 0.0627\n",
      "Epoch 5 \t Batch 91 \t Loss 0.0527\n",
      "Epoch 5 \t Batch 101 \t Loss 0.0662\n",
      "Epoch 5 \t Batch 111 \t Loss 0.0635\n",
      "Epoch 5 \t Batch 121 \t Loss 0.0675\n",
      "Epoch 5 \t Batch 131 \t Loss 0.0698\n",
      "Epoch 5 \t Batch 141 \t Loss 0.0631\n",
      "Epoch 5 \t Batch 151 \t Loss 0.0558\n",
      "Epoch 5 \t Batch 161 \t Loss 0.0592\n",
      "Epoch 5 \t Batch 171 \t Loss 0.0667\n",
      "Epoch 5 \t Batch 181 \t Loss 0.0593\n",
      "Epoch 5 \t Batch 191 \t Loss 0.0551\n",
      "Epoch 5 \t Batch 201 \t Loss 0.0629\n",
      "Epoch 5 \t Batch 211 \t Loss 0.0613\n",
      "Epoch 5 \t Batch 221 \t Loss 0.0579\n",
      "Epoch 5 \t Batch 231 \t Loss 0.0581\n",
      "Epoch 5 \t Batch 241 \t Loss 0.0625\n",
      "Epoch 5 \t Batch 251 \t Loss 0.0679\n",
      "Epoch 5 \t Batch 261 \t Loss 0.0689\n",
      "Epoch 5 \t Batch 271 \t Loss 0.0593\n",
      "Epoch 5 \t Batch 281 \t Loss 0.0668\n",
      "Epoch 5 \t Batch 291 \t Loss 0.0619\n",
      "Epoch 5 \t Batch 301 \t Loss 0.0618\n",
      "Epoch 5 \t Batch 311 \t Loss 0.0608\n",
      "Epoch 5 \t Batch 321 \t Loss 0.0634\n",
      "Epoch 5 \t Batch 331 \t Loss 0.0645\n",
      "Epoch 5 \t Batch 341 \t Loss 0.0602\n",
      "Epoch 5 \t Batch 351 \t Loss 0.0663\n",
      "Epoch 5 \t Batch 361 \t Loss 0.0671\n",
      "Epoch 5 \t Batch 371 \t Loss 0.0659\n",
      "Epoch 5 \t Batch 381 \t Loss 0.0603\n",
      "Epoch 5 \t Batch 391 \t Loss 0.0578\n",
      "Epoch 5 \t Batch 401 \t Loss 0.0632\n",
      "Epoch 5 \t Batch 411 \t Loss 0.0697\n",
      "Epoch 5 \t Batch 421 \t Loss 0.0678\n",
      "Epoch 5 \t Batch 431 \t Loss 0.0645\n",
      "Epoch 5 \t Batch 441 \t Loss 0.0657\n",
      "Epoch 5 \t Batch 451 \t Loss 0.0655\n",
      "Epoch 5 \t Batch 461 \t Loss 0.0659\n",
      "Epoch 5 \t Batch 471 \t Loss 0.0606\n",
      "Epoch 5 \t Batch 481 \t Loss 0.0711\n",
      "Epoch 5 \t Batch 491 \t Loss 0.0650\n",
      "Epoch 5 \t Batch 501 \t Loss 0.0757\n",
      "Epoch 5 \t Batch 511 \t Loss 0.0653\n",
      "Epoch 5 \t Batch 521 \t Loss 0.0705\n",
      "Epoch 5 \t Batch 531 \t Loss 0.0614\n",
      "Epoch 5 \t Batch 541 \t Loss 0.0682\n",
      "Epoch 5 \t Batch 551 \t Loss 0.0571\n",
      "Epoch 5 \t Batch 561 \t Loss 0.0597\n",
      "Epoch 5 \t Batch 571 \t Loss 0.0656\n",
      "Epoch 5 \t Batch 581 \t Loss 0.0632\n",
      "Epoch 5 \t Batch 591 \t Loss 0.0633\n",
      "Epoch 5 \t Batch 601 \t Loss 0.0580\n",
      "Epoch 5 \t Batch 611 \t Loss 0.0546\n",
      "Epoch 5 \t Batch 621 \t Loss 0.0560\n",
      "Epoch 5 \t Batch 631 \t Loss 0.0602\n",
      "Epoch 5 \t Batch 641 \t Loss 0.0670\n",
      "Epoch 5 \t Batch 651 \t Loss 0.0572\n",
      "Epoch 5 \t Batch 661 \t Loss 0.0623\n",
      "Epoch 5 \t Batch 671 \t Loss 0.0547\n",
      "Epoch 6 \t Batch 1 \t Loss 0.0628\n",
      "Epoch 6 \t Batch 11 \t Loss 0.0596\n",
      "Epoch 6 \t Batch 21 \t Loss 0.0602\n",
      "Epoch 6 \t Batch 31 \t Loss 0.0598\n",
      "Epoch 6 \t Batch 41 \t Loss 0.0614\n",
      "Epoch 6 \t Batch 51 \t Loss 0.0654\n",
      "Epoch 6 \t Batch 61 \t Loss 0.0641\n",
      "Epoch 6 \t Batch 71 \t Loss 0.0589\n",
      "Epoch 6 \t Batch 81 \t Loss 0.0625\n",
      "Epoch 6 \t Batch 91 \t Loss 0.0525\n",
      "Epoch 6 \t Batch 101 \t Loss 0.0660\n",
      "Epoch 6 \t Batch 111 \t Loss 0.0632\n",
      "Epoch 6 \t Batch 121 \t Loss 0.0672\n",
      "Epoch 6 \t Batch 131 \t Loss 0.0695\n",
      "Epoch 6 \t Batch 141 \t Loss 0.0629\n",
      "Epoch 6 \t Batch 151 \t Loss 0.0555\n",
      "Epoch 6 \t Batch 161 \t Loss 0.0589\n",
      "Epoch 6 \t Batch 171 \t Loss 0.0664\n",
      "Epoch 6 \t Batch 181 \t Loss 0.0591\n",
      "Epoch 6 \t Batch 191 \t Loss 0.0549\n",
      "Epoch 6 \t Batch 201 \t Loss 0.0627\n",
      "Epoch 6 \t Batch 211 \t Loss 0.0610\n",
      "Epoch 6 \t Batch 221 \t Loss 0.0577\n",
      "Epoch 6 \t Batch 231 \t Loss 0.0578\n",
      "Epoch 6 \t Batch 241 \t Loss 0.0623\n",
      "Epoch 6 \t Batch 251 \t Loss 0.0676\n",
      "Epoch 6 \t Batch 261 \t Loss 0.0686\n",
      "Epoch 6 \t Batch 271 \t Loss 0.0591\n",
      "Epoch 6 \t Batch 281 \t Loss 0.0665\n",
      "Epoch 6 \t Batch 291 \t Loss 0.0616\n",
      "Epoch 6 \t Batch 301 \t Loss 0.0615\n",
      "Epoch 6 \t Batch 311 \t Loss 0.0605\n",
      "Epoch 6 \t Batch 321 \t Loss 0.0632\n",
      "Epoch 6 \t Batch 331 \t Loss 0.0643\n",
      "Epoch 6 \t Batch 341 \t Loss 0.0599\n",
      "Epoch 6 \t Batch 351 \t Loss 0.0660\n",
      "Epoch 6 \t Batch 361 \t Loss 0.0668\n",
      "Epoch 6 \t Batch 371 \t Loss 0.0656\n",
      "Epoch 6 \t Batch 381 \t Loss 0.0601\n",
      "Epoch 6 \t Batch 391 \t Loss 0.0575\n",
      "Epoch 6 \t Batch 401 \t Loss 0.0630\n",
      "Epoch 6 \t Batch 411 \t Loss 0.0694\n",
      "Epoch 6 \t Batch 421 \t Loss 0.0676\n",
      "Epoch 6 \t Batch 431 \t Loss 0.0642\n",
      "Epoch 6 \t Batch 441 \t Loss 0.0654\n",
      "Epoch 6 \t Batch 451 \t Loss 0.0652\n",
      "Epoch 6 \t Batch 461 \t Loss 0.0656\n",
      "Epoch 6 \t Batch 471 \t Loss 0.0604\n",
      "Epoch 6 \t Batch 481 \t Loss 0.0708\n",
      "Epoch 6 \t Batch 491 \t Loss 0.0648\n",
      "Epoch 6 \t Batch 501 \t Loss 0.0753\n",
      "Epoch 6 \t Batch 511 \t Loss 0.0650\n",
      "Epoch 6 \t Batch 521 \t Loss 0.0702\n",
      "Epoch 6 \t Batch 531 \t Loss 0.0611\n",
      "Epoch 6 \t Batch 541 \t Loss 0.0679\n",
      "Epoch 6 \t Batch 551 \t Loss 0.0569\n",
      "Epoch 6 \t Batch 561 \t Loss 0.0595\n",
      "Epoch 6 \t Batch 571 \t Loss 0.0653\n",
      "Epoch 6 \t Batch 581 \t Loss 0.0629\n",
      "Epoch 6 \t Batch 591 \t Loss 0.0630\n",
      "Epoch 6 \t Batch 601 \t Loss 0.0578\n",
      "Epoch 6 \t Batch 611 \t Loss 0.0544\n",
      "Epoch 6 \t Batch 621 \t Loss 0.0557\n",
      "Epoch 6 \t Batch 631 \t Loss 0.0599\n",
      "Epoch 6 \t Batch 641 \t Loss 0.0667\n",
      "Epoch 6 \t Batch 651 \t Loss 0.0569\n",
      "Epoch 6 \t Batch 661 \t Loss 0.0621\n",
      "Epoch 6 \t Batch 671 \t Loss 0.0545\n",
      "Epoch 7 \t Batch 1 \t Loss 0.0626\n",
      "Epoch 7 \t Batch 11 \t Loss 0.0594\n",
      "Epoch 7 \t Batch 21 \t Loss 0.0599\n",
      "Epoch 7 \t Batch 31 \t Loss 0.0595\n",
      "Epoch 7 \t Batch 41 \t Loss 0.0612\n",
      "Epoch 7 \t Batch 51 \t Loss 0.0651\n",
      "Epoch 7 \t Batch 61 \t Loss 0.0638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 \t Batch 71 \t Loss 0.0586\n",
      "Epoch 7 \t Batch 81 \t Loss 0.0622\n",
      "Epoch 7 \t Batch 91 \t Loss 0.0522\n",
      "Epoch 7 \t Batch 101 \t Loss 0.0657\n",
      "Epoch 7 \t Batch 111 \t Loss 0.0630\n",
      "Epoch 7 \t Batch 121 \t Loss 0.0669\n",
      "Epoch 7 \t Batch 131 \t Loss 0.0692\n",
      "Epoch 7 \t Batch 141 \t Loss 0.0626\n",
      "Epoch 7 \t Batch 151 \t Loss 0.0553\n",
      "Epoch 7 \t Batch 161 \t Loss 0.0587\n",
      "Epoch 7 \t Batch 171 \t Loss 0.0661\n",
      "Epoch 7 \t Batch 181 \t Loss 0.0588\n",
      "Epoch 7 \t Batch 191 \t Loss 0.0547\n",
      "Epoch 7 \t Batch 201 \t Loss 0.0624\n",
      "Epoch 7 \t Batch 211 \t Loss 0.0608\n",
      "Epoch 7 \t Batch 221 \t Loss 0.0574\n",
      "Epoch 7 \t Batch 231 \t Loss 0.0576\n",
      "Epoch 7 \t Batch 241 \t Loss 0.0620\n",
      "Epoch 7 \t Batch 251 \t Loss 0.0673\n",
      "Epoch 7 \t Batch 261 \t Loss 0.0683\n",
      "Epoch 7 \t Batch 271 \t Loss 0.0588\n",
      "Epoch 7 \t Batch 281 \t Loss 0.0662\n",
      "Epoch 7 \t Batch 291 \t Loss 0.0613\n",
      "Epoch 7 \t Batch 301 \t Loss 0.0612\n",
      "Epoch 7 \t Batch 311 \t Loss 0.0603\n",
      "Epoch 7 \t Batch 321 \t Loss 0.0629\n",
      "Epoch 7 \t Batch 331 \t Loss 0.0640\n",
      "Epoch 7 \t Batch 341 \t Loss 0.0597\n",
      "Epoch 7 \t Batch 351 \t Loss 0.0657\n",
      "Epoch 7 \t Batch 361 \t Loss 0.0665\n",
      "Epoch 7 \t Batch 371 \t Loss 0.0653\n",
      "Epoch 7 \t Batch 381 \t Loss 0.0598\n",
      "Epoch 7 \t Batch 391 \t Loss 0.0573\n",
      "Epoch 7 \t Batch 401 \t Loss 0.0627\n",
      "Epoch 7 \t Batch 411 \t Loss 0.0691\n",
      "Epoch 7 \t Batch 421 \t Loss 0.0672\n",
      "Epoch 7 \t Batch 431 \t Loss 0.0639\n",
      "Epoch 7 \t Batch 441 \t Loss 0.0651\n",
      "Epoch 7 \t Batch 451 \t Loss 0.0649\n",
      "Epoch 7 \t Batch 461 \t Loss 0.0653\n",
      "Epoch 7 \t Batch 471 \t Loss 0.0601\n",
      "Epoch 7 \t Batch 481 \t Loss 0.0705\n",
      "Epoch 7 \t Batch 491 \t Loss 0.0645\n",
      "Epoch 7 \t Batch 501 \t Loss 0.0750\n",
      "Epoch 7 \t Batch 511 \t Loss 0.0647\n",
      "Epoch 7 \t Batch 521 \t Loss 0.0699\n",
      "Epoch 7 \t Batch 531 \t Loss 0.0608\n",
      "Epoch 7 \t Batch 541 \t Loss 0.0676\n",
      "Epoch 7 \t Batch 551 \t Loss 0.0566\n",
      "Epoch 7 \t Batch 561 \t Loss 0.0592\n",
      "Epoch 7 \t Batch 571 \t Loss 0.0650\n",
      "Epoch 7 \t Batch 581 \t Loss 0.0627\n",
      "Epoch 7 \t Batch 591 \t Loss 0.0627\n",
      "Epoch 7 \t Batch 601 \t Loss 0.0575\n",
      "Epoch 7 \t Batch 611 \t Loss 0.0542\n",
      "Epoch 7 \t Batch 621 \t Loss 0.0555\n",
      "Epoch 7 \t Batch 631 \t Loss 0.0596\n",
      "Epoch 7 \t Batch 641 \t Loss 0.0664\n",
      "Epoch 7 \t Batch 651 \t Loss 0.0567\n",
      "Epoch 7 \t Batch 661 \t Loss 0.0618\n",
      "Epoch 7 \t Batch 671 \t Loss 0.0543\n",
      "Epoch 8 \t Batch 1 \t Loss 0.0623\n",
      "Epoch 8 \t Batch 11 \t Loss 0.0591\n",
      "Epoch 8 \t Batch 21 \t Loss 0.0597\n",
      "Epoch 8 \t Batch 31 \t Loss 0.0592\n",
      "Epoch 8 \t Batch 41 \t Loss 0.0609\n",
      "Epoch 8 \t Batch 51 \t Loss 0.0648\n",
      "Epoch 8 \t Batch 61 \t Loss 0.0635\n",
      "Epoch 8 \t Batch 71 \t Loss 0.0584\n",
      "Epoch 8 \t Batch 81 \t Loss 0.0619\n",
      "Epoch 8 \t Batch 91 \t Loss 0.0520\n",
      "Epoch 8 \t Batch 101 \t Loss 0.0654\n",
      "Epoch 8 \t Batch 111 \t Loss 0.0627\n",
      "Epoch 8 \t Batch 121 \t Loss 0.0666\n",
      "Epoch 8 \t Batch 131 \t Loss 0.0689\n",
      "Epoch 8 \t Batch 141 \t Loss 0.0623\n",
      "Epoch 8 \t Batch 151 \t Loss 0.0550\n",
      "Epoch 8 \t Batch 161 \t Loss 0.0584\n",
      "Epoch 8 \t Batch 171 \t Loss 0.0658\n",
      "Epoch 8 \t Batch 181 \t Loss 0.0586\n",
      "Epoch 8 \t Batch 191 \t Loss 0.0544\n",
      "Epoch 8 \t Batch 201 \t Loss 0.0621\n",
      "Epoch 8 \t Batch 211 \t Loss 0.0605\n",
      "Epoch 8 \t Batch 221 \t Loss 0.0572\n",
      "Epoch 8 \t Batch 231 \t Loss 0.0573\n",
      "Epoch 8 \t Batch 241 \t Loss 0.0617\n",
      "Epoch 8 \t Batch 251 \t Loss 0.0670\n",
      "Epoch 8 \t Batch 261 \t Loss 0.0680\n",
      "Epoch 8 \t Batch 271 \t Loss 0.0585\n",
      "Epoch 8 \t Batch 281 \t Loss 0.0659\n",
      "Epoch 8 \t Batch 291 \t Loss 0.0611\n",
      "Epoch 8 \t Batch 301 \t Loss 0.0610\n",
      "Epoch 8 \t Batch 311 \t Loss 0.0600\n",
      "Epoch 8 \t Batch 321 \t Loss 0.0626\n",
      "Epoch 8 \t Batch 331 \t Loss 0.0637\n",
      "Epoch 8 \t Batch 341 \t Loss 0.0594\n",
      "Epoch 8 \t Batch 351 \t Loss 0.0654\n",
      "Epoch 8 \t Batch 361 \t Loss 0.0662\n",
      "Epoch 8 \t Batch 371 \t Loss 0.0650\n",
      "Epoch 8 \t Batch 381 \t Loss 0.0595\n",
      "Epoch 8 \t Batch 391 \t Loss 0.0570\n",
      "Epoch 8 \t Batch 401 \t Loss 0.0624\n",
      "Epoch 8 \t Batch 411 \t Loss 0.0688\n",
      "Epoch 8 \t Batch 421 \t Loss 0.0669\n",
      "Epoch 8 \t Batch 431 \t Loss 0.0636\n",
      "Epoch 8 \t Batch 441 \t Loss 0.0648\n",
      "Epoch 8 \t Batch 451 \t Loss 0.0646\n",
      "Epoch 8 \t Batch 461 \t Loss 0.0650\n",
      "Epoch 8 \t Batch 471 \t Loss 0.0598\n",
      "Epoch 8 \t Batch 481 \t Loss 0.0701\n",
      "Epoch 8 \t Batch 491 \t Loss 0.0642\n",
      "Epoch 8 \t Batch 501 \t Loss 0.0746\n",
      "Epoch 8 \t Batch 511 \t Loss 0.0644\n",
      "Epoch 8 \t Batch 521 \t Loss 0.0696\n",
      "Epoch 8 \t Batch 531 \t Loss 0.0605\n",
      "Epoch 8 \t Batch 541 \t Loss 0.0673\n",
      "Epoch 8 \t Batch 551 \t Loss 0.0564\n",
      "Epoch 8 \t Batch 561 \t Loss 0.0590\n",
      "Epoch 8 \t Batch 571 \t Loss 0.0647\n",
      "Epoch 8 \t Batch 581 \t Loss 0.0624\n",
      "Epoch 8 \t Batch 591 \t Loss 0.0625\n",
      "Epoch 8 \t Batch 601 \t Loss 0.0573\n",
      "Epoch 8 \t Batch 611 \t Loss 0.0539\n",
      "Epoch 8 \t Batch 621 \t Loss 0.0553\n",
      "Epoch 8 \t Batch 631 \t Loss 0.0594\n",
      "Epoch 8 \t Batch 641 \t Loss 0.0661\n",
      "Epoch 8 \t Batch 651 \t Loss 0.0564\n",
      "Epoch 8 \t Batch 661 \t Loss 0.0615\n",
      "Epoch 8 \t Batch 671 \t Loss 0.0540\n",
      "Epoch 9 \t Batch 1 \t Loss 0.0620\n",
      "Epoch 9 \t Batch 11 \t Loss 0.0588\n",
      "Epoch 9 \t Batch 21 \t Loss 0.0594\n",
      "Epoch 9 \t Batch 31 \t Loss 0.0590\n",
      "Epoch 9 \t Batch 41 \t Loss 0.0606\n",
      "Epoch 9 \t Batch 51 \t Loss 0.0645\n",
      "Epoch 9 \t Batch 61 \t Loss 0.0632\n",
      "Epoch 9 \t Batch 71 \t Loss 0.0581\n",
      "Epoch 9 \t Batch 81 \t Loss 0.0616\n",
      "Epoch 9 \t Batch 91 \t Loss 0.0518\n",
      "Epoch 9 \t Batch 101 \t Loss 0.0651\n",
      "Epoch 9 \t Batch 111 \t Loss 0.0624\n",
      "Epoch 9 \t Batch 121 \t Loss 0.0663\n",
      "Epoch 9 \t Batch 131 \t Loss 0.0686\n",
      "Epoch 9 \t Batch 141 \t Loss 0.0620\n",
      "Epoch 9 \t Batch 151 \t Loss 0.0548\n",
      "Epoch 9 \t Batch 161 \t Loss 0.0582\n",
      "Epoch 9 \t Batch 171 \t Loss 0.0655\n",
      "Epoch 9 \t Batch 181 \t Loss 0.0583\n",
      "Epoch 9 \t Batch 191 \t Loss 0.0542\n",
      "Epoch 9 \t Batch 201 \t Loss 0.0618\n",
      "Epoch 9 \t Batch 211 \t Loss 0.0602\n",
      "Epoch 9 \t Batch 221 \t Loss 0.0569\n",
      "Epoch 9 \t Batch 231 \t Loss 0.0571\n",
      "Epoch 9 \t Batch 241 \t Loss 0.0614\n",
      "Epoch 9 \t Batch 251 \t Loss 0.0667\n",
      "Epoch 9 \t Batch 261 \t Loss 0.0676\n",
      "Epoch 9 \t Batch 271 \t Loss 0.0583\n",
      "Epoch 9 \t Batch 281 \t Loss 0.0656\n",
      "Epoch 9 \t Batch 291 \t Loss 0.0608\n",
      "Epoch 9 \t Batch 301 \t Loss 0.0607\n",
      "Epoch 9 \t Batch 311 \t Loss 0.0597\n",
      "Epoch 9 \t Batch 321 \t Loss 0.0623\n",
      "Epoch 9 \t Batch 331 \t Loss 0.0634\n",
      "Epoch 9 \t Batch 341 \t Loss 0.0591\n",
      "Epoch 9 \t Batch 351 \t Loss 0.0651\n",
      "Epoch 9 \t Batch 361 \t Loss 0.0659\n",
      "Epoch 9 \t Batch 371 \t Loss 0.0647\n",
      "Epoch 9 \t Batch 381 \t Loss 0.0593\n",
      "Epoch 9 \t Batch 391 \t Loss 0.0568\n",
      "Epoch 9 \t Batch 401 \t Loss 0.0621\n",
      "Epoch 9 \t Batch 411 \t Loss 0.0685\n",
      "Epoch 9 \t Batch 421 \t Loss 0.0666\n",
      "Epoch 9 \t Batch 431 \t Loss 0.0633\n",
      "Epoch 9 \t Batch 441 \t Loss 0.0645\n",
      "Epoch 9 \t Batch 451 \t Loss 0.0643\n",
      "Epoch 9 \t Batch 461 \t Loss 0.0647\n",
      "Epoch 9 \t Batch 471 \t Loss 0.0596\n",
      "Epoch 9 \t Batch 481 \t Loss 0.0698\n",
      "Epoch 9 \t Batch 491 \t Loss 0.0639\n",
      "Epoch 9 \t Batch 501 \t Loss 0.0743\n",
      "Epoch 9 \t Batch 511 \t Loss 0.0641\n",
      "Epoch 9 \t Batch 521 \t Loss 0.0692\n",
      "Epoch 9 \t Batch 531 \t Loss 0.0603\n",
      "Epoch 9 \t Batch 541 \t Loss 0.0670\n",
      "Epoch 9 \t Batch 551 \t Loss 0.0561\n",
      "Epoch 9 \t Batch 561 \t Loss 0.0587\n",
      "Epoch 9 \t Batch 571 \t Loss 0.0644\n",
      "Epoch 9 \t Batch 581 \t Loss 0.0621\n",
      "Epoch 9 \t Batch 591 \t Loss 0.0622\n",
      "Epoch 9 \t Batch 601 \t Loss 0.0570\n",
      "Epoch 9 \t Batch 611 \t Loss 0.0537\n",
      "Epoch 9 \t Batch 621 \t Loss 0.0550\n",
      "Epoch 9 \t Batch 631 \t Loss 0.0591\n",
      "Epoch 9 \t Batch 641 \t Loss 0.0658\n",
      "Epoch 9 \t Batch 651 \t Loss 0.0562\n",
      "Epoch 9 \t Batch 661 \t Loss 0.0612\n",
      "Epoch 9 \t Batch 671 \t Loss 0.0538\n",
      "Epoch 10 \t Batch 1 \t Loss 0.0617\n",
      "Epoch 10 \t Batch 11 \t Loss 0.0586\n",
      "Epoch 10 \t Batch 21 \t Loss 0.0591\n",
      "Epoch 10 \t Batch 31 \t Loss 0.0587\n",
      "Epoch 10 \t Batch 41 \t Loss 0.0603\n",
      "Epoch 10 \t Batch 51 \t Loss 0.0642\n",
      "Epoch 10 \t Batch 61 \t Loss 0.0629\n",
      "Epoch 10 \t Batch 71 \t Loss 0.0579\n",
      "Epoch 10 \t Batch 81 \t Loss 0.0613\n",
      "Epoch 10 \t Batch 91 \t Loss 0.0516\n",
      "Epoch 10 \t Batch 101 \t Loss 0.0648\n",
      "Epoch 10 \t Batch 111 \t Loss 0.0621\n",
      "Epoch 10 \t Batch 121 \t Loss 0.0659\n",
      "Epoch 10 \t Batch 131 \t Loss 0.0682\n",
      "Epoch 10 \t Batch 141 \t Loss 0.0617\n",
      "Epoch 10 \t Batch 151 \t Loss 0.0546\n",
      "Epoch 10 \t Batch 161 \t Loss 0.0579\n",
      "Epoch 10 \t Batch 171 \t Loss 0.0652\n",
      "Epoch 10 \t Batch 181 \t Loss 0.0580\n",
      "Epoch 10 \t Batch 191 \t Loss 0.0539\n",
      "Epoch 10 \t Batch 201 \t Loss 0.0615\n",
      "Epoch 10 \t Batch 211 \t Loss 0.0599\n",
      "Epoch 10 \t Batch 221 \t Loss 0.0567\n",
      "Epoch 10 \t Batch 231 \t Loss 0.0568\n",
      "Epoch 10 \t Batch 241 \t Loss 0.0612\n",
      "Epoch 10 \t Batch 251 \t Loss 0.0664\n",
      "Epoch 10 \t Batch 261 \t Loss 0.0673\n",
      "Epoch 10 \t Batch 271 \t Loss 0.0580\n",
      "Epoch 10 \t Batch 281 \t Loss 0.0653\n",
      "Epoch 10 \t Batch 291 \t Loss 0.0605\n",
      "Epoch 10 \t Batch 301 \t Loss 0.0604\n",
      "Epoch 10 \t Batch 311 \t Loss 0.0594\n",
      "Epoch 10 \t Batch 321 \t Loss 0.0620\n",
      "Epoch 10 \t Batch 331 \t Loss 0.0631\n",
      "Epoch 10 \t Batch 341 \t Loss 0.0589\n",
      "Epoch 10 \t Batch 351 \t Loss 0.0648\n",
      "Epoch 10 \t Batch 361 \t Loss 0.0656\n",
      "Epoch 10 \t Batch 371 \t Loss 0.0644\n",
      "Epoch 10 \t Batch 381 \t Loss 0.0590\n",
      "Epoch 10 \t Batch 391 \t Loss 0.0565\n",
      "Epoch 10 \t Batch 401 \t Loss 0.0618\n",
      "Epoch 10 \t Batch 411 \t Loss 0.0681\n",
      "Epoch 10 \t Batch 421 \t Loss 0.0663\n",
      "Epoch 10 \t Batch 431 \t Loss 0.0630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 \t Batch 441 \t Loss 0.0642\n",
      "Epoch 10 \t Batch 451 \t Loss 0.0640\n",
      "Epoch 10 \t Batch 461 \t Loss 0.0644\n",
      "Epoch 10 \t Batch 471 \t Loss 0.0593\n",
      "Epoch 10 \t Batch 481 \t Loss 0.0695\n",
      "Epoch 10 \t Batch 491 \t Loss 0.0636\n",
      "Epoch 10 \t Batch 501 \t Loss 0.0739\n",
      "Epoch 10 \t Batch 511 \t Loss 0.0638\n",
      "Epoch 10 \t Batch 521 \t Loss 0.0689\n",
      "Epoch 10 \t Batch 531 \t Loss 0.0600\n",
      "Epoch 10 \t Batch 541 \t Loss 0.0667\n",
      "Epoch 10 \t Batch 551 \t Loss 0.0559\n",
      "Epoch 10 \t Batch 561 \t Loss 0.0584\n",
      "Epoch 10 \t Batch 571 \t Loss 0.0641\n",
      "Epoch 10 \t Batch 581 \t Loss 0.0618\n",
      "Epoch 10 \t Batch 591 \t Loss 0.0619\n",
      "Epoch 10 \t Batch 601 \t Loss 0.0568\n",
      "Epoch 10 \t Batch 611 \t Loss 0.0535\n",
      "Epoch 10 \t Batch 621 \t Loss 0.0548\n",
      "Epoch 10 \t Batch 631 \t Loss 0.0588\n",
      "Epoch 10 \t Batch 641 \t Loss 0.0655\n",
      "Epoch 10 \t Batch 651 \t Loss 0.0559\n",
      "Epoch 10 \t Batch 661 \t Loss 0.0609\n",
      "Epoch 10 \t Batch 671 \t Loss 0.0536\n",
      "Epoch 11 \t Batch 1 \t Loss 0.0614\n",
      "Epoch 11 \t Batch 11 \t Loss 0.0583\n",
      "Epoch 11 \t Batch 21 \t Loss 0.0589\n",
      "Epoch 11 \t Batch 31 \t Loss 0.0584\n",
      "Epoch 11 \t Batch 41 \t Loss 0.0601\n",
      "Epoch 11 \t Batch 51 \t Loss 0.0639\n",
      "Epoch 11 \t Batch 61 \t Loss 0.0626\n",
      "Epoch 11 \t Batch 71 \t Loss 0.0576\n",
      "Epoch 11 \t Batch 81 \t Loss 0.0611\n",
      "Epoch 11 \t Batch 91 \t Loss 0.0513\n",
      "Epoch 11 \t Batch 101 \t Loss 0.0645\n",
      "Epoch 11 \t Batch 111 \t Loss 0.0618\n",
      "Epoch 11 \t Batch 121 \t Loss 0.0656\n",
      "Epoch 11 \t Batch 131 \t Loss 0.0679\n",
      "Epoch 11 \t Batch 141 \t Loss 0.0615\n",
      "Epoch 11 \t Batch 151 \t Loss 0.0543\n",
      "Epoch 11 \t Batch 161 \t Loss 0.0576\n",
      "Epoch 11 \t Batch 171 \t Loss 0.0649\n",
      "Epoch 11 \t Batch 181 \t Loss 0.0578\n",
      "Epoch 11 \t Batch 191 \t Loss 0.0537\n",
      "Epoch 11 \t Batch 201 \t Loss 0.0613\n",
      "Epoch 11 \t Batch 211 \t Loss 0.0597\n",
      "Epoch 11 \t Batch 221 \t Loss 0.0564\n",
      "Epoch 11 \t Batch 231 \t Loss 0.0566\n",
      "Epoch 11 \t Batch 241 \t Loss 0.0609\n",
      "Epoch 11 \t Batch 251 \t Loss 0.0660\n",
      "Epoch 11 \t Batch 261 \t Loss 0.0670\n",
      "Epoch 11 \t Batch 271 \t Loss 0.0578\n",
      "Epoch 11 \t Batch 281 \t Loss 0.0649\n",
      "Epoch 11 \t Batch 291 \t Loss 0.0602\n",
      "Epoch 11 \t Batch 301 \t Loss 0.0601\n",
      "Epoch 11 \t Batch 311 \t Loss 0.0592\n",
      "Epoch 11 \t Batch 321 \t Loss 0.0617\n",
      "Epoch 11 \t Batch 331 \t Loss 0.0628\n",
      "Epoch 11 \t Batch 341 \t Loss 0.0586\n",
      "Epoch 11 \t Batch 351 \t Loss 0.0645\n",
      "Epoch 11 \t Batch 361 \t Loss 0.0653\n",
      "Epoch 11 \t Batch 371 \t Loss 0.0641\n",
      "Epoch 11 \t Batch 381 \t Loss 0.0587\n",
      "Epoch 11 \t Batch 391 \t Loss 0.0562\n",
      "Epoch 11 \t Batch 401 \t Loss 0.0615\n",
      "Epoch 11 \t Batch 411 \t Loss 0.0678\n",
      "Epoch 11 \t Batch 421 \t Loss 0.0660\n",
      "Epoch 11 \t Batch 431 \t Loss 0.0627\n",
      "Epoch 11 \t Batch 441 \t Loss 0.0639\n",
      "Epoch 11 \t Batch 451 \t Loss 0.0637\n",
      "Epoch 11 \t Batch 461 \t Loss 0.0641\n",
      "Epoch 11 \t Batch 471 \t Loss 0.0590\n",
      "Epoch 11 \t Batch 481 \t Loss 0.0691\n",
      "Epoch 11 \t Batch 491 \t Loss 0.0633\n",
      "Epoch 11 \t Batch 501 \t Loss 0.0735\n",
      "Epoch 11 \t Batch 511 \t Loss 0.0635\n",
      "Epoch 11 \t Batch 521 \t Loss 0.0686\n",
      "Epoch 11 \t Batch 531 \t Loss 0.0597\n",
      "Epoch 11 \t Batch 541 \t Loss 0.0664\n",
      "Epoch 11 \t Batch 551 \t Loss 0.0556\n",
      "Epoch 11 \t Batch 561 \t Loss 0.0582\n",
      "Epoch 11 \t Batch 571 \t Loss 0.0638\n",
      "Epoch 11 \t Batch 581 \t Loss 0.0615\n",
      "Epoch 11 \t Batch 591 \t Loss 0.0616\n",
      "Epoch 11 \t Batch 601 \t Loss 0.0565\n",
      "Epoch 11 \t Batch 611 \t Loss 0.0532\n",
      "Epoch 11 \t Batch 621 \t Loss 0.0545\n",
      "Epoch 11 \t Batch 631 \t Loss 0.0586\n",
      "Epoch 11 \t Batch 641 \t Loss 0.0652\n",
      "Epoch 11 \t Batch 651 \t Loss 0.0557\n",
      "Epoch 11 \t Batch 661 \t Loss 0.0607\n",
      "Epoch 11 \t Batch 671 \t Loss 0.0533\n",
      "Epoch 12 \t Batch 1 \t Loss 0.0611\n",
      "Epoch 12 \t Batch 11 \t Loss 0.0581\n",
      "Epoch 12 \t Batch 21 \t Loss 0.0586\n",
      "Epoch 12 \t Batch 31 \t Loss 0.0582\n",
      "Epoch 12 \t Batch 41 \t Loss 0.0598\n",
      "Epoch 12 \t Batch 51 \t Loss 0.0636\n",
      "Epoch 12 \t Batch 61 \t Loss 0.0623\n",
      "Epoch 12 \t Batch 71 \t Loss 0.0573\n",
      "Epoch 12 \t Batch 81 \t Loss 0.0608\n",
      "Epoch 12 \t Batch 91 \t Loss 0.0511\n",
      "Epoch 12 \t Batch 101 \t Loss 0.0642\n",
      "Epoch 12 \t Batch 111 \t Loss 0.0615\n",
      "Epoch 12 \t Batch 121 \t Loss 0.0653\n",
      "Epoch 12 \t Batch 131 \t Loss 0.0676\n",
      "Epoch 12 \t Batch 141 \t Loss 0.0612\n",
      "Epoch 12 \t Batch 151 \t Loss 0.0541\n",
      "Epoch 12 \t Batch 161 \t Loss 0.0574\n",
      "Epoch 12 \t Batch 171 \t Loss 0.0646\n",
      "Epoch 12 \t Batch 181 \t Loss 0.0575\n",
      "Epoch 12 \t Batch 191 \t Loss 0.0535\n",
      "Epoch 12 \t Batch 201 \t Loss 0.0610\n",
      "Epoch 12 \t Batch 211 \t Loss 0.0594\n",
      "Epoch 12 \t Batch 221 \t Loss 0.0562\n",
      "Epoch 12 \t Batch 231 \t Loss 0.0563\n",
      "Epoch 12 \t Batch 241 \t Loss 0.0606\n",
      "Epoch 12 \t Batch 251 \t Loss 0.0657\n",
      "Epoch 12 \t Batch 261 \t Loss 0.0667\n",
      "Epoch 12 \t Batch 271 \t Loss 0.0575\n",
      "Epoch 12 \t Batch 281 \t Loss 0.0646\n",
      "Epoch 12 \t Batch 291 \t Loss 0.0600\n",
      "Epoch 12 \t Batch 301 \t Loss 0.0598\n",
      "Epoch 12 \t Batch 311 \t Loss 0.0589\n",
      "Epoch 12 \t Batch 321 \t Loss 0.0614\n",
      "Epoch 12 \t Batch 331 \t Loss 0.0625\n",
      "Epoch 12 \t Batch 341 \t Loss 0.0583\n",
      "Epoch 12 \t Batch 351 \t Loss 0.0642\n",
      "Epoch 12 \t Batch 361 \t Loss 0.0650\n",
      "Epoch 12 \t Batch 371 \t Loss 0.0638\n",
      "Epoch 12 \t Batch 381 \t Loss 0.0585\n",
      "Epoch 12 \t Batch 391 \t Loss 0.0560\n",
      "Epoch 12 \t Batch 401 \t Loss 0.0613\n",
      "Epoch 12 \t Batch 411 \t Loss 0.0675\n",
      "Epoch 12 \t Batch 421 \t Loss 0.0657\n",
      "Epoch 12 \t Batch 431 \t Loss 0.0625\n",
      "Epoch 12 \t Batch 441 \t Loss 0.0636\n",
      "Epoch 12 \t Batch 451 \t Loss 0.0634\n",
      "Epoch 12 \t Batch 461 \t Loss 0.0638\n",
      "Epoch 12 \t Batch 471 \t Loss 0.0588\n",
      "Epoch 12 \t Batch 481 \t Loss 0.0688\n",
      "Epoch 12 \t Batch 491 \t Loss 0.0630\n",
      "Epoch 12 \t Batch 501 \t Loss 0.0732\n",
      "Epoch 12 \t Batch 511 \t Loss 0.0632\n",
      "Epoch 12 \t Batch 521 \t Loss 0.0682\n",
      "Epoch 12 \t Batch 531 \t Loss 0.0594\n",
      "Epoch 12 \t Batch 541 \t Loss 0.0660\n",
      "Epoch 12 \t Batch 551 \t Loss 0.0554\n",
      "Epoch 12 \t Batch 561 \t Loss 0.0579\n",
      "Epoch 12 \t Batch 571 \t Loss 0.0635\n",
      "Epoch 12 \t Batch 581 \t Loss 0.0613\n",
      "Epoch 12 \t Batch 591 \t Loss 0.0613\n",
      "Epoch 12 \t Batch 601 \t Loss 0.0563\n",
      "Epoch 12 \t Batch 611 \t Loss 0.0530\n",
      "Epoch 12 \t Batch 621 \t Loss 0.0543\n",
      "Epoch 12 \t Batch 631 \t Loss 0.0583\n",
      "Epoch 12 \t Batch 641 \t Loss 0.0649\n",
      "Epoch 12 \t Batch 651 \t Loss 0.0554\n",
      "Epoch 12 \t Batch 661 \t Loss 0.0604\n",
      "Epoch 12 \t Batch 671 \t Loss 0.0531\n",
      "Epoch 13 \t Batch 1 \t Loss 0.0609\n",
      "Epoch 13 \t Batch 11 \t Loss 0.0578\n",
      "Epoch 13 \t Batch 21 \t Loss 0.0583\n",
      "Epoch 13 \t Batch 31 \t Loss 0.0579\n",
      "Epoch 13 \t Batch 41 \t Loss 0.0595\n",
      "Epoch 13 \t Batch 51 \t Loss 0.0633\n",
      "Epoch 13 \t Batch 61 \t Loss 0.0621\n",
      "Epoch 13 \t Batch 71 \t Loss 0.0571\n",
      "Epoch 13 \t Batch 81 \t Loss 0.0605\n",
      "Epoch 13 \t Batch 91 \t Loss 0.0509\n",
      "Epoch 13 \t Batch 101 \t Loss 0.0639\n",
      "Epoch 13 \t Batch 111 \t Loss 0.0612\n",
      "Epoch 13 \t Batch 121 \t Loss 0.0650\n",
      "Epoch 13 \t Batch 131 \t Loss 0.0673\n",
      "Epoch 13 \t Batch 141 \t Loss 0.0609\n",
      "Epoch 13 \t Batch 151 \t Loss 0.0538\n",
      "Epoch 13 \t Batch 161 \t Loss 0.0571\n",
      "Epoch 13 \t Batch 171 \t Loss 0.0643\n",
      "Epoch 13 \t Batch 181 \t Loss 0.0573\n"
     ]
    }
   ],
   "source": [
    "ops.reset_default_graph()\n",
    "global sess\n",
    "config = tf.ConfigProto()\n",
    "sess = tf.Session(config = config)\n",
    "graph = tf.get_default_graph()\n",
    "# params['lrn_rate'] = 0.0009\n",
    "Autoencoder_Model()\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
